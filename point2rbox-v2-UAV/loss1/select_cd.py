import os
import glob
import math
import numpy as np
import cv2
import torch
from tqdm import tqdm
import multiprocessing

# ================= é…ç½®åŒºåŸŸ =================
# CODrone æ•°æ®é›†æ ¹è·¯å¾„
DATASET_ROOT = '/mnt/data/xiekaikai/split_ss_codrone'
# éœ€è¦è®¡ç®—çš„å­é›†åç§°
TARGET_SPLITS = ['trainval', 'test']

# æœç´¢çš„ K å€¼åˆ—è¡¨
K_LIST = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 
          1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 
          2.2, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]

# CODrone ç±»åˆ«å®šä¹‰ (12ç±»)
CLASSES = ('car', 'truck', 'bus', 'traffic-light',
           'traffic-sign', 'bridge', 'people', 'bicycle',
           'motor', 'tricycle', 'boat', 'ship')

# é‡‡æ ·æ–‡ä»¶æ•°é‡ (-1 è¡¨ç¤ºè·‘å…¨é‡)
SAMPLE_NUM = -1 

# å¼ºåˆ¶ä½¿ç”¨ CPU è¿›è¡ŒæŒ‡æ ‡è®¡ç®—
DEVICE = 'cpu' 
# ===========================================

# ç±»åˆ«æ˜ å°„è¡¨
CLS_MAP = {c: i for i, c in enumerate(CLASSES)}

def parse_codrone_file(file_path):
    """
    è§£æ CODrone æ ¼å¼ txt æ–‡ä»¶
    """
    bboxes = []
    labels = []
    
    if not os.path.exists(file_path):
        return None, None

    with open(file_path, 'r') as f:
        lines = f.readlines()
    
    for line in lines:
        parts = line.strip().split()
        if len(parts) < 9: continue
        
        try:
            poly = np.array([float(x) for x in parts[:8]]).reshape(4, 2).astype(np.float32)
            cls_name = parts[8]
        except ValueError:
            continue
            
        if cls_name not in CLS_MAP:
            continue
            
        rect = cv2.minAreaRect(poly)
        (cx, cy), (w, h), angle = rect
        
        if w < h:
            w, h = h, w
            angle += 90
        
        theta = np.deg2rad(angle)
        
        bboxes.append([cx, cy, w, h, theta])
        labels.append(CLS_MAP[cls_name])
        
    if not bboxes:
        return None, None
        
    # ã€ä¿®å¤æ ¸å¿ƒã€‘è¿”å› numpy æ•°ç»„ï¼Œè€Œä¸æ˜¯ torch.tensor
    # è¿™é¿å…äº†å¤šè¿›ç¨‹å…±äº«å†…å­˜å¥æŸ„è€—å°½çš„é—®é¢˜
    return np.array(bboxes, dtype=np.float32), np.array(labels, dtype=np.int64)

@torch.no_grad()
def compute_naoa_metrics(k_radius, dataset_samples):
    """
    ä½¿ç”¨æŒ‡å®šçš„ k_radius åœ¨æ•°æ®é›†ä¸Šè¿è¡Œ NAOALoss V4 è®¡ç®—æµç¨‹
    """
    total_chaos_sum = 0.0
    total_valid_samples = 0
    total_isolated_count = 0
    total_objects = 0
    total_neighbor_sum = 0.0
    
    # è¿™é‡Œçš„ dataset_samples é‡Œå­˜çš„æ˜¯ numpy array
    for bboxes_np, labels_np in tqdm(dataset_samples, desc=f"è®¡ç®— K={k_radius:<4}", leave=False, dynamic_ncols=True):
        if bboxes_np is None or len(bboxes_np) < 2:
            continue
            
        # ã€ä¿®å¤æ ¸å¿ƒã€‘åœ¨ä¸»è¿›ç¨‹è®¡ç®—å‰ï¼Œå°† numpy è½¬å› tensor
        bboxes = torch.from_numpy(bboxes_np).to(DEVICE)
        labels = torch.from_numpy(labels_np).to(DEVICE)
        
        N = bboxes.shape[0]
        total_objects += N
        
        # ================= Step 1: å‡ ä½•è§£è€¦ =================
        centers = bboxes[:, :2]
        wh = bboxes[:, 2:4]
        scales = (wh[:, 0] * wh[:, 1]).sqrt().clamp(min=16.0, max=800.0)
        thetas = bboxes[:, 4]
        
        # ================= Step 2: çŸ¢é‡åŒ– (4-Theta) =================
        vecs = torch.stack([torch.cos(4 * thetas), torch.sin(4 * thetas)], dim=1)
        
        # ================= Step 3: æ„å»ºäº²å’ŒçŸ©é˜µ =================
        dist_sq = torch.cdist(centers, centers, p=2).pow(2)
        sigmas = scales * k_radius 
        sigma_mat = sigmas.view(N, 1)
        
        # Gaussian Kernel
        W_geo = torch.exp(-dist_sq / (2 * sigma_mat.pow(2)))
        
        # 3.3 é€»è¾‘æ©ç  (ä»…åŒç±»)
        mask_cls = (labels.view(N, 1) == labels.view(1, N)).float()
        
        # ç»„åˆæƒé‡ (åŒ…å«è‡ªç¯)
        W = W_geo * mask_cls
        
        # --- ç»Ÿè®¡æŒ‡æ ‡è®¡ç®— ---
        W_no_diag = W.clone()
        W_no_diag.fill_diagonal_(0)
        neighbor_strength = W_no_diag.sum(dim=1)
        
        is_isolated = neighbor_strength < 0.1
        total_isolated_count += is_isolated.sum().item()
        total_neighbor_sum += neighbor_strength.sum().item()
        
        # ================= Step 4: å½’ä¸€åŒ– =================
        W_sum = W.sum(dim=1, keepdim=True)
        W_norm = W / W_sum
        
        # ================= Step 5: èƒ½é‡è®¡ç®— =================
        mean_vecs = torch.mm(W_norm, vecs)
        chaos_score = 1.0 - mean_vecs.norm(dim=1)
        
        valid_mask = ~is_isolated
        if valid_mask.sum() > 0:
            total_chaos_sum += chaos_score[valid_mask].sum().item()
            total_valid_samples += valid_mask.sum().item()
            
    # æ±‡æ€»å…¨å±€æŒ‡æ ‡
    avg_chaos = total_chaos_sum / max(1, total_valid_samples)
    isolation_rate = total_isolated_count / max(1, total_objects)
    avg_neighbors = total_neighbor_sum / max(1, total_objects)
    
    return avg_chaos, isolation_rate, avg_neighbors

def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½ CODrone æ•°æ®é›† (åŒ…å«: {', '.join(TARGET_SPLITS)}) ...")
    
    all_files = []
    for split in TARGET_SPLITS:
        split_path = os.path.join(DATASET_ROOT, split, 'labelTxt')
        print(f"  - æ‰«æ {split} é›†: {split_path} ...")
        
        if not os.path.exists(split_path):
            print(f"    [è­¦å‘Š] è·¯å¾„ä¸å­˜åœ¨: {split_path}")
            continue
            
        files = glob.glob(os.path.join(split_path, '*.txt'))
        print(f"    -> æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        all_files.extend(files)
    
    print(f"æ€»è®¡æ‰¾åˆ° {len(all_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶ã€‚")
    
    if len(all_files) == 0:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return
        
    use_files = all_files
    if SAMPLE_NUM != -1 and SAMPLE_NUM < len(all_files):
        print(f"è­¦å‘Š: ä»£ç è®¾ç½®ä¸ºå…¨é‡è®¡ç®—ï¼Œå¿½ç•¥ SAMPLE_NUM={SAMPLE_NUM}")
        
    print(f"ä½¿ç”¨å…¨éƒ¨ {len(use_files)} ä¸ªæ–‡ä»¶è¿›è¡Œå…¨é‡è®¡ç®—...")
    print(f"âš ï¸  æç¤ºï¼šæ•°æ®åŠ è½½å·²ä¼˜åŒ–ä¸º Numpy æ¨¡å¼ï¼Œé¿å…å†…å­˜é”™è¯¯ã€‚")
        
    # é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜
    dataset_samples = []
    print("æ­£åœ¨é¢„å¤„ç†æ ‡æ³¨æ•°æ® (è¿™å¯èƒ½éœ€è¦ä¸€åˆ†é’Ÿ)...")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæ–‡ä»¶è¯»å–
    pool = multiprocessing.Pool(processes=min(16, multiprocessing.cpu_count()))
    for b, l in tqdm(pool.imap(parse_codrone_file, use_files, chunksize=100), total=len(use_files)):
        if b is not None:
            dataset_samples.append((b, l))
    pool.close()
    pool.join()
    
    print(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°: {len(dataset_samples)}")
    
    print(f"\n{'='*80}")
    print(f"å¼€å§‹ K_RADIUS å‚æ•°æœç´¢ (K_LIST: {K_LIST})")
    print(f"è¡¡é‡æ ‡å‡†: å¯»æ‰¾å­¤ç«‹ç‡(Iso)è¾ƒä½ï¼Œä¸”å¹³å‡æ··ä¹±åº¦(Chaos)ä¹Ÿè¾ƒä½çš„å¹³è¡¡ç‚¹")
    print(f"{'='*80}")
    print(f"{'K-Radius':<10} | {'Avg Chaos':<12} | {'Isolation%':<12} | {'Avg Neighbors':<15}")
    print("-" * 80)
    
    results = []
    
    for k in K_LIST:
        avg_chaos, iso_rate, avg_neigh = compute_naoa_metrics(k, dataset_samples)
        
        # å®æ—¶æ‰“å°ç»“æœ
        print(f"{k:<10.1f} | {avg_chaos:<12.4f} | {iso_rate*100:<11.2f}% | {avg_neigh:<15.2f}")
        
        results.append({
            'k': k,
            'chaos': avg_chaos,
            'iso': iso_rate
        })
        
    print("-" * 80)
    
    # === è‡ªåŠ¨æ¨èé€»è¾‘ ===
    candidates = [r for r in results if r['iso'] < 0.15]
    
    if not candidates:
        print("\n[åˆ†æ] æ•°æ®é›†éå¸¸ç¨€ç–ï¼Œå³ä½¿ K å¾ˆå¤§å­¤ç«‹ç‡ä¾ç„¶å¾ˆé«˜ã€‚")
        best = min(results, key=lambda x: x['iso']) 
        print(f"[æ¨è] å»ºè®®ä½¿ç”¨è¾ƒå¤§çš„ K = {best['k']} (å­¤ç«‹ç‡ {best['iso']*100:.1f}%)")
    else:
        best = min(candidates, key=lambda x: x['chaos'])
        print(f"\n[æ¨è] æœ€ä¼˜ K_RADIUS = {best['k']}")
        print(f"  ç†ç”±: åœ¨æ»¡è¶³è¦†ç›–ç‡(å­¤ç«‹ç‡ < 15%)çš„å‰æä¸‹ï¼Œ")
        print(f"        è¯¥å‚æ•°èƒ½ä¿æŒæœ€ä½çš„å†…éƒ¨æ··ä¹±åº¦ ({best['chaos']:.4f})ï¼Œ")
        print(f"        è¯´æ˜é‚»åŸŸå†…çš„ç‰©ä½“æ—¢ä¸°å¯Œåˆæ•´é½ã€‚")

if __name__ == "__main__":
    main()