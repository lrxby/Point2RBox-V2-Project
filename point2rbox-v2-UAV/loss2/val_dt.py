import os
import glob
import numpy as np
import pandas as pd
from tqdm import tqdm
import multiprocessing

# ================= é…ç½®åŒºåŸŸ =================
# 1. DOTAæ•°æ®é›†è·¯å¾„ï¼ˆä¸¥æ ¼æŒ‰ç…§ä½ æä¾›çš„ç›®å½•ç»“æ„ï¼‰
DOTA_ROOT = '/mnt/data/xiekaikai/split_ss_dota'
FULL_TRAINVAL_DIR = os.path.join(DOTA_ROOT, 'trainval', 'labelTxt')
# DOTAæ— ç‹¬ç«‹æµ‹è¯•é›†ï¼Œç›´æ¥åˆ é™¤è¯¥é…ç½®ï¼Œæ— éœ€ä¿ç•™

# 2. å¯¹æ¯”æ¨¡å¼ï¼ˆä¿æŒä¸å˜ï¼‰
MODES = ['log', 'linear', 'sqrt', 'square']

# 3. DOTAæ•°æ®é›†å®˜æ–¹ç±»åˆ«ï¼ˆå®Œå…¨ä½¿ç”¨ä½ æä¾›çš„åˆ—è¡¨ï¼‰
CLASSES = (
    'plane', 'baseball-diamond', 'bridge', 'ground-track-field',
    'small-vehicle', 'large-vehicle', 'ship', 'tennis-court',
    'basketball-court', 'storage-tank', 'soccer-ball-field', 'roundabout',
    'harbor', 'swimming-pool', 'helicopter'
)
RIDGE_LAMBDA = 1e-4  # ä¸ Loss ä¿æŒä¸€è‡´
EPS = 1e-6           # ä¸ Loss çš„ clamp ä¿æŒä¸€è‡´

# 4. ç»“æœä¿å­˜è·¯å¾„ï¼ˆåŒºåˆ†æ•°æ®é›†ï¼Œé¿å…è¦†ç›–åŸæœ‰ç»“æœï¼‰
SAVE_DIR = '/mnt/data/liurunxiang/workplace/point2rbox-v2-UAV/loss2/val/DOTA'

# 5. å¤šè¿›ç¨‹é…ç½®ï¼šè‡ªåŠ¨è·å–CPUæ ¸å¿ƒæ•°ï¼Œé¢„ç•™2æ ¸ä¿è¯ç³»ç»Ÿç¨³å®š
WORKER_NUM = max(1, multiprocessing.cpu_count() - 2)
# ===========================================

def polygon_area(coords):
    x = coords[0::2]
    y = coords[1::2]
    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))

def parse_txt(txt_path):
    """è§£æDOTAæ ¼å¼GTæ•°æ®ï¼š8åæ ‡ + ç±»åˆ«å + éš¾åº¦å€¼"""
    bboxes = []
    labels = []
    
    with open(txt_path, 'r') as f:
        lines = f.readlines()
        for line in lines:
            parts = line.strip().split()
            # DOTAæ ¼å¼ï¼š8ä¸ªæµ®ç‚¹åæ ‡ + ç±»åˆ«åç§° + éš¾åº¦æ•°å­—ï¼Œæ€»é•¿åº¦â‰¥10
            if len(parts) < 10:
                continue
            # æå–ç±»åˆ«åç§°ï¼ˆç¬¬9åˆ—ï¼Œç´¢å¼•8ï¼‰
            cls_name = parts[8]
            # è¿‡æ»¤ä¸åœ¨åˆæ³•ç±»åˆ«åˆ—è¡¨ä¸­çš„ç›®æ ‡
            if cls_name not in CLASSES:
                continue
            
            # æå–8ä¸ªåæ ‡å€¼å¹¶è½¬æ¢ä¸ºæµ®ç‚¹å‹
            coords = list(map(float, parts[:8]))
            area = polygon_area(coords)
            # è¿‡æ»¤æå°é¢ç§¯ç›®æ ‡
            if area <= 1:
                continue 
            
            s = np.sqrt(area)
            s = max(s, 1e-2)
            
            # è®¡ç®—å››ç‚¹ä¸­å¿ƒåæ ‡
            cx = sum(coords[0::2]) / 4.0
            cy = sum(coords[1::2]) / 4.0
            
            bboxes.append([cx, cy, s, s]) # [x, y, w, h]
            labels.append(cls_name)
            
    return np.array(bboxes), labels

def fit_by_loss_logic(bboxes, labels_str, mode='log'):
    """æŸå¤±è®¡ç®—é€»è¾‘ï¼ˆå®Œå…¨ä¸å˜ï¼‰"""
    if len(bboxes) == 0: return None
    
    # === 1. å‡†å¤‡æ•°æ® ===
    x_c = bboxes[:, 0]
    y_c = bboxes[:, 1]
    w = bboxes[:, 2]
    h = bboxes[:, 3]
    
    # ç‰©ç†çœŸå€¼ (Ground Truth Size)
    s_gt = np.sqrt(w * h)

    # === æ„é€  Target Y ===
    if mode == 'log':
        Y = 0.5 * np.log(w * h)
    elif mode == 'linear':
        Y = s_gt
    elif mode == 'sqrt':
        Y = np.sqrt(s_gt)
    elif mode == 'square':
        Y = w * h
    
    # === 2. æ£€æŸ¥çº¦æŸ ===
    unique_labels = sorted(list(set(labels_str)))
    K = len(unique_labels)
    N = len(bboxes)
    
    if N < K + 3: return None

    cls_to_idx = {name: i for i, name in enumerate(unique_labels)}
    labels_idx = np.array([cls_to_idx[name] for name in labels_str])

    # === 3. å½’ä¸€åŒ– (Z-Score) ===
    x_mean, x_std = np.mean(x_c), np.std(x_c)
    y_mean, y_std = np.mean(y_c), np.std(y_c)
    x_std = max(x_std, EPS)
    y_std = max(y_std, EPS)
    x_norm = (x_c - x_mean) / x_std
    y_norm = (y_c - y_mean) / y_std

    # === 4. æ„å»ºçŸ©é˜µ A ===
    A = np.zeros((N, 2 + K))
    A[:, 0] = x_norm
    A[:, 1] = y_norm
    for i, idx in enumerate(labels_idx):
        A[i, 2 + idx] = 1.0

    # === 5. æ±‚è§£ theta ===
    M = A.T @ A
    I_reg = np.eye(2 + K) * RIDGE_LAMBDA
    try:
        theta = np.linalg.inv(M + I_reg) @ (A.T @ Y)
    except np.linalg.LinAlgError:
        return None

    # === 6. é¢„æµ‹ä¸è¿˜åŸ (Inverse Transform) ===
    Y_hat = A @ theta
    
    if mode == 'log':
        s_pred = np.exp(Y_hat)
    elif mode == 'linear':
        s_pred = Y_hat
    elif mode == 'sqrt':
        s_pred = np.maximum(Y_hat, 0) ** 2
    elif mode == 'square':
        s_pred = np.sqrt(np.maximum(Y_hat, 0))
        
    # === 7. è®¡ç®—å¤šç§è¯¯å·®æŒ‡æ ‡ (æŒ‰ç‰©ä½“) ===
    # 7.1 ç»å¯¹è¯¯å·® MAE (px)
    diff_abs = np.abs(s_gt - s_pred)
    
    # 7.2 ç›¸å¯¹è¯¯å·® MAPE (%)
    diff_rel = diff_abs / (s_gt + EPS)
    
    # 7.3 å‡æ–¹è¯¯å·® MSE (px^2)
    diff_mse = (s_gt - s_pred) ** 2
    
    # 7.4 Size IoU (0~1)
    area_gt = s_gt ** 2
    area_pred = s_pred ** 2
    area_gt = np.maximum(area_gt, EPS)
    area_pred = np.maximum(area_pred, EPS)
    
    inter = np.minimum(area_gt, area_pred)
    union = np.maximum(area_gt, area_pred)
    iou = inter / union
    
    return diff_abs, diff_rel, diff_mse, iou, labels_str

# ================= å•æ–‡ä»¶å¹¶è¡Œå¤„ç†å‡½æ•° =================
def process_single_file(txt_path):
    """å°è£…å•ä¸ªæ–‡ä»¶çš„å®Œæ•´å¤„ç†é€»è¾‘ï¼ˆå®Œå…¨ä¸å˜ï¼‰"""
    try:
        bboxes, labels = parse_txt(txt_path)
        current_res = {}
        # éå†æ‰€æœ‰æ¨¡å¼è®¡ç®—
        for mode in MODES:
            res = fit_by_loss_logic(bboxes, labels, mode)
            if res is None:
                return None
            current_res[mode] = res
        return current_res
    except Exception:
        return None

# ================= å¤šè¿›ç¨‹æ•°æ®æ”¶é›†å‡½æ•° =================
def collect_dataset_stats(ann_dir, dataset_name):
    """å¤šè¿›ç¨‹å¹¶è¡Œæ”¶é›†æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆå®Œå…¨ä¸å˜ï¼‰"""
    print(f"\nğŸš€ æ­£åœ¨åˆ†ææ•°æ®é›†: {dataset_name} ...")
    # å…¼å®¹ç›®å½•æ ¡éªŒé€»è¾‘
    if not os.path.exists(ann_dir):
        print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {ann_dir}ï¼Œè·³è¿‡è¯¥æ•°æ®é›†")
        metrics = ['abs', 'rel', 'mse', 'iou']
        return {m: {c: {met: [] for met in metrics} for c in CLASSES} for m in MODES}
        
    txt_files = glob.glob(os.path.join(ann_dir, '*.txt'))
    
    # å­˜å‚¨ç»“æ„
    metrics = ['abs', 'rel', 'mse', 'iou']
    stats = {m: {c: {met: [] for met in metrics} for c in CLASSES} for m in MODES}
    
    valid_count = 0
    # å¤šè¿›ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡ä»¶
    with multiprocessing.Pool(WORKER_NUM) as pool:
        # å¹¶è¡Œæ‰§è¡Œ + è¿›åº¦æ¡å±•ç¤º
        results = list(tqdm(
            pool.imap(process_single_file, txt_files),
            total=len(txt_files),
            desc=f"Fitting {dataset_name}"
        ))

    # èšåˆæ‰€æœ‰æœ‰æ•ˆè®¡ç®—ç»“æœ
    for res in results:
        if res is not None:
            valid_count += 1
            # æŒ‰æ¨¡å¼ã€ç±»åˆ«å½’æ¡£æŒ‡æ ‡æ•°æ®
            for mode in MODES:
                d_abs, d_rel, d_mse, d_iou, obj_labels = res[mode]
                for i, cls in enumerate(obj_labels):
                    stats[mode][cls]['abs'].append(d_abs[i])
                    stats[mode][cls]['rel'].append(d_rel[i])
                    stats[mode][cls]['mse'].append(d_mse[i])
                    stats[mode][cls]['iou'].append(d_iou[i])

    print(f"âœ… {dataset_name} æœ‰æ•ˆå›¾ç‰‡æ•°: {valid_count}")
    return stats

def print_and_save_stats(stats, title="Dataset Report", save_filename="report.txt"):
    """æ‰“å°ä¸ä¿å­˜TXTæŠ¥å‘Šï¼ˆå®Œå…¨ä¸å˜ï¼‰"""
    summary_data = []
    
    # éå†æ¯ä¸ªç±»åˆ«æ„å»ºæ•°æ®è¡Œ
    for cls in CLASSES:
        row = {'Class': cls}
        sample_count = len(stats['log'][cls]['abs'])
        row['Samples'] = sample_count
        
        if sample_count == 0:
            for mode in MODES:
                row[f'{mode}_IoU'] = np.nan
                row[f'{mode}_MAE'] = np.nan
                row[f'{mode}_MAPE'] = np.nan
                row[f'{mode}_MSE'] = np.nan
            summary_data.append(row)
            continue

        for mode in MODES:
            row[f'{mode}_IoU'] = np.mean(stats[mode][cls]['iou']) * 100
            row[f'{mode}_MAE'] = np.mean(stats[mode][cls]['abs'])
            row[f'{mode}_MAPE'] = np.mean(stats[mode][cls]['rel']) * 100
            row[f'{mode}_MSE'] = np.mean(stats[mode][cls]['mse'])
            
        summary_data.append(row)
        
    # è®¡ç®—å…¨å±€æ±‡æ€»æŒ‡æ ‡
    total_row = {'Class': 'GLOBAL_ALL', 'Samples': 0}
    for mode in MODES:
        all_metrics = {'abs':[], 'rel':[], 'mse':[], 'iou':[]}
        for cls in CLASSES:
            for met in all_metrics:
                all_metrics[met].extend(stats[mode][cls][met])
        
        total_row['Samples'] = len(all_metrics['abs'])
        if total_row['Samples'] > 0:
            total_row[f'{mode}_IoU'] = np.mean(all_metrics['iou']) * 100
            total_row[f'{mode}_MAE'] = np.mean(all_metrics['abs'])
            total_row[f'{mode}_MAPE'] = np.mean(all_metrics['rel']) * 100
            total_row[f'{mode}_MSE'] = np.mean(all_metrics['mse'])
        else:
            total_row[f'{mode}_IoU'] = np.nan
            total_row[f'{mode}_MAE'] = np.nan
            total_row[f'{mode}_MAPE'] = np.nan
            total_row[f'{mode}_MSE'] = np.nan
        
    summary_data.append(total_row)
    
    df = pd.DataFrame(summary_data)
    # è°ƒæ•´åˆ—é¡ºåº
    cols = ['Class', 'Samples']
    for met in ['IoU', 'MAE', 'MAPE', 'MSE']:
        for mode in MODES:
            cols.append(f'{mode}_{met}')
    df = df[cols]

    # é…ç½®æ ¼å¼åŒ–è¾“å‡ºå‚æ•°
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 2000)
    pd.set_option('display.float_format', '{:.2f}'.format)
    
    # æ„å»ºå®Œæ•´è¾“å‡ºæ–‡æœ¬å†…å®¹
    split_line = "="*160
    content = f"\n{split_line}\nğŸ“Š {title}\n{split_line}\n"
    content += df.to_string(index=False)
    content += f"\n{split_line}\n"

    # æ§åˆ¶å°æ‰“å°
    print(content)

    # ä¿å­˜ä¸ºæ ¼å¼åŒ–TXTæ–‡ä»¶
    save_path = os.path.join(SAVE_DIR, save_filename)
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\nğŸ’¾ æŠ¥è¡¨å·²ä¿å­˜è‡³: {save_path}")

def main():
    # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(SAVE_DIR, exist_ok=True)
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³ç›®å½•: {SAVE_DIR}")
    print(f"âš¡ å¤šè¿›ç¨‹åŠ é€Ÿå·²å¯ç”¨ï¼Œå·¥ä½œè¿›ç¨‹æ•°ï¼š{WORKER_NUM}")
    
    # ä»…ç»Ÿè®¡DOTAå”¯ä¸€çš„è®­ç»ƒéªŒè¯é›†ï¼Œæ— æµ‹è¯•é›†/åˆå¹¶é€»è¾‘
    stats_trainval = collect_dataset_stats(FULL_TRAINVAL_DIR, "TrainVal Set")
    
    # ä»…ç”Ÿæˆå¹¶ä¿å­˜ä¸€ä»½æ•°æ®é›†æŠ¥å‘Š
    print_and_save_stats(
        stats_trainval,
        title="è¯¦ç»†å¯¹æ¯”æŠ¥å‘Š: TrainVal Set | æŒ‡æ ‡ï¼šIoU(â†‘), MAE(px)(â†“), MAPE(%)(â†“), MSE(â†“)",
        save_filename="trainval_set_report.txt"
    )

    # ç»“æœè§£è¯»æç¤ºï¼ˆä¿æŒä¸å˜ï¼‰
    print("\nğŸ’¡ ç»“æœè§£è¯»å»ºè®®:")
    print("1. [IoU]: æœ€é‡è¦çš„å‡†ç¡®æ€§æŒ‡æ ‡ã€‚")
    print("2. [MAE]: å¹³å‡åƒç´ è¯¯å·®ã€‚Log æ¨¡å¼è‹¥åœ¨æ­¤æŒ‡æ ‡ä¸Šä¹Ÿé¢†å…ˆï¼Œè¯´æ˜å®ƒä¸ä»…æ¯”ä¾‹å‡†ï¼Œç»å¯¹å€¼ä¹Ÿå‡†ã€‚")
    print("3. [MAPE]: ç›¸å¯¹è¯¯å·®ï¼Œä½“ç°å¯¹å°ç›®æ ‡çš„å‹å¥½ç¨‹åº¦ã€‚")
    print("4. [MSE]: å¯¹ç¦»ç¾¤ç‚¹æ•æ„Ÿã€‚")

if __name__ == '__main__':
    # é€‚é…Linuxç³»ç»Ÿï¼Œè®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼
    multiprocessing.set_start_method('fork', force=True)
    main()